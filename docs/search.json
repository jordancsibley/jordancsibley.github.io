[
  {
    "objectID": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html",
    "href": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html",
    "title": "Remote Sensing Analysis of the Thomas Fire",
    "section": "",
    "text": "Los Padres National Forest, December 2017"
  },
  {
    "objectID": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#about",
    "href": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#about",
    "title": "Remote Sensing Analysis of the Thomas Fire",
    "section": "About",
    "text": "About\nThis project provides an in-depth analysis of the Thomas Fire, which burned over 280,000 acres in Ventura and Santa Barbara counties in December 2017. The first part of the analysis examines air quality impacts using Air Quality Index (AQI) data from the US Environmental Protection Agency, visualizing how air quality changed over time during and after the fire. The second part focuses on the fire’s physical impact, leveraging Landsat 8 satellite imagery and historic fire perimeter data. By applying false-color imaging techniques, this analysis highlights fire scars and assesses vegetation health in the affected areas."
  },
  {
    "objectID": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#highlights-of-analysis",
    "href": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#highlights-of-analysis",
    "title": "Remote Sensing Analysis of the Thomas Fire",
    "section": "Highlights of Analysis",
    "text": "Highlights of Analysis\n\nData Wrangling: A portion of the project involves cleaning and preparing the data for analysis. This includes updating data types of the AQI data and dropping the band dimension for the Landsat dataset for better usability.\nWorking with different data types: This analysis utilizes many different data types which required unique packages for reading in. The AQI data was read in using pandas from an online repository, while the spatial data required rioxarray and geopandas.\nFalse Color Imaging: The notebook explores how false color images can highlight burn areas using shortwave infrared (SWIR) and near-infrared (NIR) bands to distinguish between burned land and healthy vegetation.\n\nSee more about my analyses on the GitHub repository for this project."
  },
  {
    "objectID": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#data",
    "href": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#data",
    "title": "Remote Sensing Analysis of the Thomas Fire",
    "section": "Data",
    "text": "Data\n\nAir Quality Index (AQI) Data: The AQI data comes from the US Environmental Protection Agency and includes the daily air quality index score for Santa Barbara County.\nLandsat 8 Data: The Landsat data used in this analysis is the landsat8-2018-01-26-sb-simplified.nc dataset, which includes key spectral bands (e.g., shortwave infrared, near-infrared, and red) for the Santa Barbara region. This data is used to create both true-color and false-color images, with specific bands selected to highlight different land features.\nFire Perimeter Data: The fire perimeter data comes from CAL FIRE and is used to overlay the Thomas Fire’s boundary on the imagery. The selected dataset includes the fire perimeter for the 2017 Thomas Fire which was extracted from the larger database and then saved as a shapefile. The process of selecting this fire perimeter can be seen in this Jupyter notebook.\n\n\nSet Up\n\n\nShow code\n# Import libraries \nimport pandas as pd\nimport numpy as np\nimport os \n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nimport geopandas as gpd \nimport rioxarray as rioxr\nimport matplotlib.patches as mpatches # For custom legend creation\nfrom matplotlib_scalebar.scalebar import ScaleBar \n\n# Show all columns \npd.set_option(\"display.max.columns\", None)\n\n\n\n\nImport data\n\n\nShow code\n# ---- AQI data for 2017-2018 \n\n# Read in data\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip', compression='zip')\naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip', compression='zip')\n\n# Concatnate the 'aqi_17' and 'aqi_18' dataframes into a single dataframe\naqi = pd.concat([aqi_17, aqi_18])\n\n# ---- Landsat data \n\n# Path to data in folder \nlandsat_fp = os.path.join(os.getcwd(), 'data', 'landsat8-2018-01-26-sb-simplified.nc')\n\n# Open with rioxarray\nlandsat = rioxr.open_rasterio(landsat_fp)\n\n# ---- Thomas fire perimeter \n\n# Path to data in file  \nthomas_fp = os.path.join(os.getcwd(), 'data', 'thomas.shp')\n\n# Read in Thomas fire perimeter data\nthomas = gpd.read_file(thomas_fp)"
  },
  {
    "objectID": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#aqi-analysis",
    "href": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#aqi-analysis",
    "title": "Remote Sensing Analysis of the Thomas Fire",
    "section": "AQI Analysis",
    "text": "AQI Analysis\n\nClean Data\nIn order to use the dataset that includes the daily AQI, the following steps were taken:\n\nClean column names: The columns were converted to lower snake case through the functions .str.lower() and .str.replace() to remove the spaces in between the words. This allows for better readability and compatibility with certain Python functions.\nFilter to SB County: The dataset included AQI scores for all counties in the U.S. It needed to be filtered to just include the county of interest.\nDrop columns: Columns were removed using .drop() in order to only leave the columns needed for analysis.\nConvert date column: The original date column was in the data type object. By using pandas.to_datetime it was changed to a datetime data type which will make it easier to extract certain dates and for creating a time series graph.\n\n\n\nShow code\n# Clean column names \naqi.columns = (aqi.columns\n                  .str.lower()\n                  .str.replace(' ','_'))\n\n# Filter to Santa Barbara County \naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\n\n# Drop columns relating to state and county info \naqi_sb = aqi_sb.drop(['state_name', 'county_name', 'state_code', 'county_code'], axis=1)\n\n# Change 'date' column from object to datetime data type\naqi_sb.date = pd.to_datetime(aqi_sb['date'])\n\n# Update the index of the dataframe to be the 'date' column \naqi_sb = aqi_sb.set_index('date')\n\n\n\n\nCalculate Rolling Averages\nWhile the data provides the daily AQI score, it is sometimes better for visibility in a plot if an average is calculated over a certain time range. This will make the plot easier to read by minimizing the impact by outliers to viewing the overall trend. For this particular plot, the rolling 5 day average was calculated using the pandas method rolling() and mean() to find the average AQI score over the 5 day window.\n\n# Calculate AQI rolling average over 5 days\nrolling_average = aqi_sb['aqi'].rolling('5D').mean()\n\n# Add rolling average values to new column in data frame \naqi_sb['five_day_average'] = rolling_average\n\n\n\nPlot AQI for 2017 through 2018\nNow that the data is cleaned and the rolling average has been calculated, a plot of daily and average AQI can be plotted from January 2017 to January 2019.\n\n\nShow code\n# Establish plot boundaries\nplt.figure()\n\n# Line plot of daily aqi \nplt.plot(aqi_sb.index, aqi_sb['aqi'], label='Daily AQI', color='blue', linewidth=1)\n\n# Line plot of 5 day moving averages \nplt.plot(aqi_sb.index, aqi_sb['five_day_average'], label='5-Day Average AQI', color='red', linewidth=2)\n\n# Add labels and title\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('AQI', fontsize=12)\nplt.title('Santa Barbara County Air Quality Index (2017-2018)', fontsize=14)\n\n# Fix the x-axis to show only month and last 2 digits of the year\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b-%y'))\n\n# Add a legend \nplt.legend()\n\n\n\n\n\n\n\n\n\nLooking at the plot, there is a large spike in the daily and average AQI score right before January 2018. The Thomas Fire took place from December 4, 2017, until its containment on January 12, 2018 so it is reasonable to see a higher AQI score during this time period due to the smoke and air contaminants as a result of the fire."
  },
  {
    "objectID": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#spatial-analysis",
    "href": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#spatial-analysis",
    "title": "Remote Sensing Analysis of the Thomas Fire",
    "section": "Spatial Analysis",
    "text": "Spatial Analysis\nTo visualize the extent of the Thomas Fire, Landsat 8 imagery can be used to view satellite images in their true (Red Green Blue bands) colors and also by visualizing the shortwave infrared bands and near infrared bands. These bands are able to better analyze the damage done by fires than just a satellite image due to their distinct responses to vegetation, soil, and water content.\nLandsat data consists of satellite imagery collected by the Landsat program, a series of Earth-observing satellites jointly managed by NASA and the U.S. Geological Survey (USGS). Landsat sensors capture data in multiple spectral bands, including visible light, near-infrared (NIR), shortwave infrared (SWIR), and thermal infrared (TIR).\nHere are articles that go into more depth about Landsat data use:\n\nNASA Earth Observatory - Why is that Forest Red and that Cloud Blue? How to Interpret a False-Color Satellite Image\nWhat are the band designations for the Landsat satellites?\nCommon Landsat Band Combinations\n\n\nData Exploration\n\n\nShow code\n# View data elements \nprint('Height: ', landsat.rio.height)\nprint('Width: ', landsat.rio.width)\nprint('Size of dimensions: ', dict(landsat.sizes))\nprint('Spatial bounding box:')\nprint(landsat.rio.bounds(), '\\n')\nprint('CRS: ', landsat.rio.crs)\n\n\nHeight:  731\nWidth:  870\nSize of dimensions:  {'band': 1, 'x': 870, 'y': 731}\nSpatial bounding box:\n(121170.0, 3755160.0, 356070.0, 3952530.0) \n\nCRS:  EPSG:32611\n\n\nThe landsat data is an xarray that contains the band information. The dimensions are x and y and also band that just contains the integer 1. The data variables of the xarray are the band types which are red, green, blue, near infrared (nir08), and the shortwave infrared (swir22)."
  },
  {
    "objectID": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#data-cleaning",
    "href": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#data-cleaning",
    "title": "Remote Sensing Analysis of the Thomas Fire",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nSince the band dimension of the data has only one value 1 it is unnecessary to keep and is best practice to remove. Using the xarray.DataArray methods squeeze() and drop_vars() executes this by removing any dimensions of size 1 and then removing the variable band from the dataset.\n\n\nShow code\n# Drop the `band` dimension of the data and remove dims of length 1 \nlandsat = landsat.squeeze().drop_vars('band')\n\n# View altered landsat data \nlandsat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 25MB\nDimensions:      (x: 870, y: 731)\nCoordinates:\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0\nData variables:\n    red          (y, x) float64 5MB ...\n    green        (y, x) float64 5MB ...\n    blue         (y, x) float64 5MB ...\n    nir08        (y, x) float64 5MB ...\n    swir22       (y, x) float64 5MB ...xarray.DatasetDimensions:x: 870y: 731Coordinates: (3)x(x)float641.213e+05 1.216e+05 ... 3.559e+05axis :Xcrs :EPSG:32611long_name :x coordinate of projectionresolution :30standard_name :projection_x_coordinateunits :metre_FillValue :nanarray([121305., 121575., 121845., ..., 355395., 355665., 355935.])y(y)float643.952e+06 3.952e+06 ... 3.755e+06axis :Ycrs :EPSG:32611long_name :y coordinate of projectionresolution :-30standard_name :projection_y_coordinateunits :metre_FillValue :nanarray([3952395., 3952125., 3951855., ..., 3755835., 3755565., 3755295.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]GeoTransform :121170.0 270.0 0.0 3952530.0 0.0 -270.0array(0)Data variables: (5)red(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]green(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]blue(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]nir08(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]swir22(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]Indexes: (2)xPandasIndexPandasIndex(Index([121305.0, 121575.0, 121845.0, 122115.0, 122385.0, 122655.0, 122925.0,\n       123195.0, 123465.0, 123735.0,\n       ...\n       353505.0, 353775.0, 354045.0, 354315.0, 354585.0, 354855.0, 355125.0,\n       355395.0, 355665.0, 355935.0],\n      dtype='float64', name='x', length=870))yPandasIndexPandasIndex(Index([3952395.0, 3952125.0, 3951855.0, 3951585.0, 3951315.0, 3951045.0,\n       3950775.0, 3950505.0, 3950235.0, 3949965.0,\n       ...\n       3757725.0, 3757455.0, 3757185.0, 3756915.0, 3756645.0, 3756375.0,\n       3756105.0, 3755835.0, 3755565.0, 3755295.0],\n      dtype='float64', name='y', length=731))Attributes: (0)\n\n\nNow we are left with the dimensions x and y which are coordinates that hold spectral band information needed to produce the landsat map images."
  },
  {
    "objectID": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#true-color-rgb-image",
    "href": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#true-color-rgb-image",
    "title": "Remote Sensing Analysis of the Thomas Fire",
    "section": "True Color (RGB) Image",
    "text": "True Color (RGB) Image\nBy selecting the visual bands red, green, and blue and placing them in their true channels, we are able to see an image with colors that you would expect in the environment.\n\n# Adjust scale to get true color image \nlandsat[['red', 'green', 'blue']].to_array().plot.imshow(robust=True)"
  },
  {
    "objectID": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#false-color-image",
    "href": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#false-color-image",
    "title": "Remote Sensing Analysis of the Thomas Fire",
    "section": "False Color Image",
    "text": "False Color Image\nSince you are able to select which bands go in each channel, you can use the colors to view different spectral bands. In the following map, shortwave infrared (SWIR) is placed in the red channel, near infrared (NIR) is placed in the green channel, and red wavelength is placed in the blue channel.\n\n# Create false color image with short infrared, near infrared and red \nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True)\n\n\n\n\n\n\n\n\nThe RGB image and False color image were produced by selecting certian bands to go in channels that display those colors. Notice in the two maps the robust parameter within plot.imshow(). When set to True, it adjusts the color scale to exclude outlines and improving visualization.\nNow let’s bring it all together and include the fire boundary that I extracted from the CAL FIRE database. Before I can use the fire boundary in my map, I need to ensure it is in the same coordinate reference system. The following code reprojects the CRS if need be before moving forward.\n\n# Ensure CRSs match (and reproject if necessary)\nif landsat.rio.crs != thomas.crs:\n    thomas = thomas.to_crs(landsat.rio.crs)\n\n# Validate the CRSs match\nassert landsat.rio.crs == thomas.crs\n\nWith the perimeter data CRS now set to match the landsat data, they can be plotted in the same map.\nHere are some highlights of the map creation:\n\nmatplotlib.patches is utilized to create a custom legend that displays what colors correspond to which bands. This sub module provides various geometric shapes which can be added to plots for annotating or highlighting specific areas, but I find it helpful to use to create legends for plots with non-standard markers.\nScalebar() is used to include a 50 km scale bar which gives the map a relative size and also provide more context to the extent of the fire’s range.\n\n\n\nShow code\n# Plot false color image with thomas fire boundary \nfig, ax= plt.subplots(figsize = (10, 10), facecolor='white')\n\n# False color landsat image \nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True,\n                                                           ax=ax\n                                                          )\n\n# Overlay Thomas Fire boundary\nthomas.boundary.plot(ax=ax,\n                     color = \"maroon\",\n                     linewidth=2)\n\n# Create custom legend for short wave and infrared \nswir_patch = mpatches.Patch(color='#FD8A75', label='Short Wave Infrared')\nnir_patch = mpatches.Patch(color='#67FF5B', label='Near Infrared')\n\n\n# Set legend position \nax.legend(handles=[swir_patch, nir_patch], loc='upper right', title='Bands')\n\n\n# Set perimeter label \nax.text(\n    x=287070.0, y=3832030.0,  # Coordinates \n    s=\"Thomas Fire Perimeter\",  # Label text\n    color='maroon', fontsize=8, weight='bold',  \n    bbox=dict(facecolor='white', alpha=0.5, edgecolor='none') \n    \n)\n# Set scale bar \nscalebar = ScaleBar(1, units='m', location='lower left', length_fraction=0.25, scale_loc='bottom', color='black')  \nax.add_artist(scalebar)\n\n\n# Set title                                                           \nax.set_title(\"Santa Barbara and Ventura County, CA (2018-01-26)\", weight='bold')\n\n# Remove axes ticks\nax.set_xticks([])  \nax.set_yticks([])  \n\n# Remove axes labels\nax.set_xlabel(\"\")  \nax.set_ylabel(\"\")  \n\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure Description: This map highlights the area of Santa Barbara and Ventura counties affected by the Thomas Fire, which burned over 280,000 acres from December 4, 2017, until its containment on January 12, 2018. The fire’s total burn perimeter is outlined in dark red. The false-color image incorporates shortwave infrared (SWIR) in red, which is particularly effective for identifying areas of burn damage, as newly burned land strongly reflects SWIR wavelengths. The near-infrared (NIR) band is represented in green, highlighting healthy vegetation, as plants strongly reflect NIR.\nIn conclusion, visualizing AQI scores during the Thomas Fire and mapping its burn scar using SWIR provides valuable insights into the fire’s impact on Santa Barbara and Ventura counties. However, it is important to note that these analyses primarily illustrate large-scale effects and do not address the fire’s direct impacts on people and the environment at the ground level."
  },
  {
    "objectID": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#references-and-data-sources",
    "href": "posts/2024-12-03-thomas-fire/thomas-fire-blog.html#references-and-data-sources",
    "title": "Remote Sensing Analysis of the Thomas Fire",
    "section": "References and Data Sources:",
    "text": "References and Data Sources:\nEnvironmental Protection Agency (EPA). Air Quality Index (AQI) Data. https://aqs.epa.gov/aqsweb/airdata/download_files.html (Accessed October, 2024)\nEarth Resources Observation and Science (EROS) Center. (2020). Landsat 8-9 Operational Land Imager / Thermal Infrared Sensor Level-2, Collection 2 [dataset]. U.S. Geological Survey. https://doi.org/10.5066/P9OGBGM6 (Access Novemeber, 2024)\nCalifornia Department of Forestry and Fire Protection (CAL FIRE). (2023). California fire perimeters (all). Data.gov. https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436 (Accessed November, 2024)"
  },
  {
    "objectID": "posts/2024-12-10-fish-stats/index.html",
    "href": "posts/2024-12-10-fish-stats/index.html",
    "title": "Exploring the Impacts of Dredging on Fish Abundance",
    "section": "",
    "text": "Carpinteria Salt Marsh. Source: Channel Islands Restoration"
  },
  {
    "objectID": "posts/2024-12-10-fish-stats/index.html#background",
    "href": "posts/2024-12-10-fish-stats/index.html#background",
    "title": "Exploring the Impacts of Dredging on Fish Abundance",
    "section": "Background",
    "text": "Background\nWetlands are among the most ecologically important ecosystems, providing habitat for a wide range of species, supporting biodiversity, and delivering critical ecosystem services such as flood control and water filtration. In California, wetlands are especially vital due to their rarity and the pressures they face from human development and climate change.\nOne particularly important wetland is the Carpinteria Salt Marsh, located in southern California. The Carpinteria Salt Marsh Reserve, managed by the University of California, is home to a critically important estuary that supports a variety of sensitive plant and animal species [1]. The marsh encompasses wetland, subtidal channels, and upland habitats, and it serves as a regional nursery for species such as the California halibut. Additionally, it provides refuge for migratory birds and species of special concern. Adjacent to a sandy beach, rocky reefs, and kelp beds, the marsh plays an integral role in the local ecosystem.\nIn the winter of 2023, severe rain storms deposited a significant amount of sediment into the marsh, requiring emergency dredging operations led by Santa Barbara County Flood Control [2]. While necessary for flood mitigation, dredging can have significant ecological impacts, including increased turbidity, altered hydrology, and stress to plant and animal life [3]. From April to July 2023, a hydraulic dredge operated 24/7 to remove sediment and reduce the risk of flooding to the surrounding area.\nHaving worked in the Carpinteria Salt Marsh during the fall of 2023 as part of the SONGS Marine Mitigation team, I witnessed firsthand the changes caused by the dredging. Conversations with my colleagues often touched on concerns about the county’s approach to dredging, particularly regarding the lack of consultation with scientists and the arbitrary depth choices made during the operation. These experiences, combined with my curiosity to apply the statistical skills I’ve gained through my stats course (EDS 222), inspired me to explore the question:\n\nIs there a difference in fish abundance between the pre-dredging (2022) and post-dredging (2023) field seasons?\n\n\n\n\nDredging in the Carpinteria Salt Marsh, April 2023. Source: Santa Barbara County Public Works Department"
  },
  {
    "objectID": "posts/2024-12-10-fish-stats/index.html#set-up",
    "href": "posts/2024-12-10-fish-stats/index.html#set-up",
    "title": "Exploring the Impacts of Dredging on Fish Abundance",
    "section": "Set Up",
    "text": "Set Up\nThe packages needed for this project in R are here for reading in the data, and tidyverse and dplyr for data cleaning and visualization.\n\n\nSee code\n# Load important packages \nlibrary(tidyverse)\nlibrary(here)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/2024-12-10-fish-stats/index.html#data",
    "href": "posts/2024-12-10-fish-stats/index.html#data",
    "title": "Exploring the Impacts of Dredging on Fish Abundance",
    "section": "Data",
    "text": "Data\nThis data used in this project comes from the SONGS Marine Mitigation database.The SONGS (San Onofre Nuclear Generating Station) San Dieguito Wetland Restoration mitigation monitoring program was designed to evaluate compliance of the restoration project with conditions of the SONGS permit as set out by the California Coastal Commission. To evaluate wetland performance of the restoration site, three comparison sites were selected: Tijuana Estuary, Mugu Lagoon, and Carpinteria Salt Marsh. Since 2012, yearly biological surveys have taken place to quantify water quality parameters and vegetation, fish, and invertebrate populations.\nThe particular data I selected to use from this database are water quality and fish abundance data [4] [5]. The water quality data contains measurements of temperature, salinity, and dissolved oxygen that were collected by an in situ sensor that records data daily at 15 minute intervals.\nThe fish abundance data includes the fish sampled during a technique called seining. Seining involves spreading out a fine mesh net to enclose a specific area of the channel or tidal creek and then another net is passed through the enclosed area to collect whichever fish are found on the bottom or in the waterway. Each wetland contained 12 different sites where seining was done; 6 sites were in a main channel and 6 sites in a tidal creek, and every site was sampled three times in one sampling season. I found this to be one of my favorite jobs while working for SONGS. It was really cool to bring the net up and see which kinds of fish were there. My favorite fish we would get were the California halibut and the Pacific staghorn sculpin, but most commonly we would get were topsmelt and California killifish.\n\n\n\nSONGS sampling team seining for fish at Mugu Lagoon. Source: Instagram @ ucsb_marinemitigation\n\n\nIf you want to access the data or explore the metadata, use the links below:\n\nWater Quality Data\nFish Abundance Data\n\n\n\nSee code\n# Load in data\nwater_quality &lt;- read_csv(here('posts', '2024-12-10-fish-stats', 'data', 'wetland_ts_water_quality.csv'))\n\nfish_abund &lt;- read_csv(here('posts', '2024-12-10-fish-stats', 'data', 'wetland_ts_fish_seine.csv'))"
  },
  {
    "objectID": "posts/2024-12-10-fish-stats/index.html#data-cleaning",
    "href": "posts/2024-12-10-fish-stats/index.html#data-cleaning",
    "title": "Exploring the Impacts of Dredging on Fish Abundance",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nThe Plan:\nIn order to use the datasets for my analysis, I needed to complete a series of data cleaning tasks. My end goal was to transform the two dataset into one that contained the total number of fish sampled at each site at each day, and the average water quality parameters of that day.\nTo accomplish this I had to make a series of choices. The first being how to aggregate the water quality data. The sensors collected readings every 15 minutes, resulting in a massive dataset that wasn’t practical for my analysis, especially since I needed to join it with the fish abundance data. One issue I realized was that the fish abundance data gave the date but not the time the fish were sampled. This is where having worked in the field proved invaluable. I decided that if I didn’t have an exact time, I would just find the average of the water quality parameters during the time I knew sampling took place. The time of sampling differed a bit each day of field season because the we had to do the field work during a specific tide window and it had to be during the daylight hours so that we could see (plus my bosses never liked us working too late in the afternoon). This knowledge led me to subset the water quality data from 6 AM until 4 PM, and then I found the average values of that day. While wetlands are a very dynamic environment with water quality parameters often changing quickly with the tides, I thought subsetting to the sampling hours was a bit better than just taking the average of the entire day.\nThe next choice I had to make was on how I was going to present the fish abundance data. The original dataset included the species of fish and number sampled for each seine haul at every site. For a particular survey date, 2-3 sites would be seined and each site had 6 hauls. Every site had a A side and then a B side directly next to each other. Once the nets were set up, each seine area had 3 hauls of a net being passed through and the contents identified and counted. In order to get the total number of fished sampled at the site, I grouped the data by site (combining A and B) and by the date. This grouping made it easier to merge the fish data with the corresponding water quality data by date.\nThe last step I decided to take was to find the density of the fish sampled rather than just keep it at total count. My rational with this was that for each seine, the distance across remained the same, but the total length and depth of the water varied by creek width or depth of channel. As the nets were set up and pulled by only six people, we wouldn’t set up the seine in areas were it was too deep to stand as it made it nearly impossible to pull the net across while keeping it in contact with the ground. To standardize for this uneven sampling area, I decide to calculate the total area of water sampled by multiplying the seine dimensions and average depth of the water. Then, I could calculate the density of fish sampled per meter squared by dividing the total fish count by the area. Once I started using this value as my fish abundance variable, I found it was much more consistent across the different sites. Using raw count values without considering the area might have lead to misleading results, as larger sampled areas usually led to higher counts.\nHere are the steps I took in my code to clean the data and prepare it for merging:\nFor Both data sets:\n\nSubset to years 2022 and 2023\nFilter to Carpinteria Salt Marsh (CSM)\n\nFor water quality data:\n\nFilter to survey hours (6am to 4pm) to get an average value for water quality parameters during the day\nCreate new column with only the date, not the time, to prepare for joining later\nCalculate the average value over the time period for each water quality variable using the group_by() function for the date and then summarise() and mean() to find the average value of the daylight hours.\n\nFor fish abundance data\n\nRemove unnecessary columns\nCreate new column that gives water sampling area (seine area * depth)\nFind fish count per meter sampling area\nGroup by date and site to find the total fish density for given site/day using group_by() and summarise().\nCreate a binary dredging column that gives a 0 value for the pre-dredging year (2022) and a 1 for the post-dredging year (2023)\n\n\n\nSee code\n# ---- Water quality data cleaning \n# Filter water quality data\nwater_quality_clean &lt;- water_quality %&gt;% \n  filter(year %in% c(2022, 2023)) %&gt;% \n  filter(wetland_code == \"CSM\") %&gt;% \n  select(-c(instrument_type, wetland_code)) %&gt;% \n  filter(hour(sample_datetime_gmt) &gt;= 6 & hour(sample_datetime_gmt)) %&gt;%  # filter to sampling hours \n  mutate(date = as.Date(sample_datetime_gmt)) # new column with just date\n\n# Calculate averages \nwater_quality_ave &lt;- water_quality_clean %&gt;% \n  group_by(date) %&gt;% \n  summarise(temp_c_mean = mean(temperature_degrees_c),\n            salinity_ppt_mean = mean(salinity_ppt), \n            salinity_practical_mean = mean(salinity_practical),\n            dissolved_oxygen_mean = mean(dissolved_oxygen_concentration_mg_l))  \n\n# ---- Fish survey data cleaning \n# Filter fish abundance \nfish_abund_clean &lt;- fish_abund %&gt;% \n  filter(year %in% c(2022, 2023)) %&gt;% \n  filter(wetland_code == \"CSM\") %&gt;% \n  select(-c(survey, module_code, seine_section_code, seine_sample_length, seine_sample_width, species_id)) %&gt;%  # remove unnecessary columns\n  mutate(sample_area_m2 = seine_sample_area * depth) %&gt;%  # create count per m2\n  mutate(count_per_m2 = count /sample_area_m2)\n\n# Group by date and site name to get total count of fish sampled\nfish_total &lt;- fish_abund_clean %&gt;% \n  group_by(date, tc_mc_code, habitat_code) %&gt;% \n  summarise(total_density_per_m2 = sum(count_per_m2, na.rm = TRUE)) %&gt;%\n  mutate(habitat_code = recode(habitat_code,\n                          \"TC\" = \"Tidal creek\",\n                          \"BNMC\" = \"Main channel\")\n  )\n\n# Create a new column for the year\nfish_total$year &lt;- year(fish_total$date)\n\n# Create binary column for pre dredging (2022) and post dredging (2023)\nfish_total$dredging &lt;- ifelse(fish_total$year == 2023, 1, 0)\n\n\n\n\nJoin Data\nNow that I have the average water quality values, I joined it with the fish abundance data so that for each sample day, there is corresponding water quality data.\n\nfish_water &lt;- merge(fish_total, water_quality_ave, by=\"date\")\n\nWhen I merged the data it only has 67 rows when I was expecting it to have 72 to match the fish abundance data. As a test, I decided to do a left join of the fish data to keep all its dates intact and discover which dates were missing water quality data.\n\n\nSee code\n# Perform a left join\nfish_with_water_quality &lt;- fish_total %&gt;%\n  left_join(water_quality_ave, by = \"date\")\n\n# Check which rows in fish_total do not have a match in water_quality_ave\nmissing_dates &lt;- fish_with_water_quality %&gt;%\n  filter(is.na(temp_c_mean)) \n\n# View the missing dates\nmissing_dates\n\n\n# A tibble: 5 × 10\n# Groups:   date, tc_mc_code [5]\n  date       tc_mc_code habitat_code total_density_per_m2  year dredging\n  &lt;date&gt;     &lt;chr&gt;      &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 2023-09-06 CSM MC3    Main channel              0.0144   2023        1\n2 2023-09-06 CSM MC4    Main channel              0.0292   2023        1\n3 2023-09-06 CSM TC3    Tidal creek               0.00594  2023        1\n4 2023-09-11 CSM MC2    Main channel              0.00621  2023        1\n5 2023-09-11 CSM TC2    Tidal creek               0.0694   2023        1\n# ℹ 4 more variables: temp_c_mean &lt;dbl&gt;, salinity_ppt_mean &lt;dbl&gt;,\n#   salinity_practical_mean &lt;dbl&gt;, dissolved_oxygen_mean &lt;dbl&gt;\n\n\nThe test showed that there was no water quality data for the dates 2023-09-06 and 2023-09-11. This meant that for 5 fish collection dates in the 2023 field season, there was no water quality data to accompany it. After checking the original water quality data it showed that there is a gap in data from 09-10 until 09-14. I couldn’t find a way to get that missing data, so I was forced to just exclude those sample dates from my analysis. It is important to recognize that any plots or tests in the rest of the project are comparing 36 sites for 2022 and 31 sites for 2023. While this is not ideal, I felt it was a small enough difference that I went on with my analysis."
  },
  {
    "objectID": "posts/2024-12-10-fish-stats/index.html#data-visualization",
    "href": "posts/2024-12-10-fish-stats/index.html#data-visualization",
    "title": "Exploring the Impacts of Dredging on Fish Abundance",
    "section": "Data visualization",
    "text": "Data visualization\nAfter the data was cleaned, I created some plots to compare the fish density between the two years and see if there is a potential relationship with the water quality parameters.\n\nBoxplot to compare fish density between years.\nThe log scale was applied to the y-axis because the data contains a wide range of fish density values, including many small values and only a few larger ones. This disparity can make it difficult to distinguish differences in densities, as the larger values dominate the visualization. The log transformation compresses the range of values, allowing for better visualization of variations while maintaining the relative differences.\n\n\nSee code\n# Box plot of fish density (log scaled)\nggplot(fish_total, aes(x = factor(year), y = total_density_per_m2, fill = factor(year))) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"#8C510A\", \"#01665E\")) +\n  scale_y_log10() +  # Apply log scale to y-axis\n  labs(\n    title = \"Fish Total Density per m² (Log Scale)\",\n    x = \"Year\",\n    y = \"Total Density per m² (log scale)\",\n    fill = \"Year\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nLooking at the two box plots, the overall range of the values seemed to be pretty similar. I would assume just based on this that there is no difference in the fish density between 2022 and 2023, but the linear models would give a better understand if that is true.\n\n\nFish density with water qualtity parameters\nTo explore if the water quality parameter (temperature, salinity, and dissolved oxygen) had a potential relationship with fish density, each variable is plotted against the fish densities and a linear model line was included to view the general trends.\n\n\nSee code\n# ---- Fish density vs. each water quality  variable\n\n# water temp \nggplot(fish_water, aes(x = temp_c_mean, y = total_density_per_m2, color = factor(dredging))) + \n  geom_point(alpha = 0.7) + \n  geom_smooth(method = \"lm\", se = TRUE, lwd = 0.7) +\n  scale_color_manual(values = c(\"0\" = \"#A6611A\", \"1\" = \"#5AB4AC\"),\n                     labels = c(\"Pre-Dredging\", \"Post-Dredging\")) +\n  labs(color = \"Dredging Status\",\n       x = \"Average water temperature (C)\",\n       y = \"Fish density per m2\",\n       title = \"Water Temp vs Fish Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSee code\n# dissolved oxygen \nggplot(fish_water, aes(x = dissolved_oxygen_mean, y = total_density_per_m2, color = factor(dredging))) + \n  geom_point(alpha = 0.7) + \n  geom_smooth(method = \"lm\", se = TRUE, lwd = 0.7) +\n  scale_color_manual(values = c(\"0\" = \"#A6611A\", \"1\" = \"#5AB4AC\"),\n                     labels = c(\"Pre-Dredging\", \"Post-Dredging\")) +\n  labs(color = \"Dredging Status\",\n       x = \"Average dissolved oxygen concentration (mg/L)\",\n       y = \"Fish density per m2\",\n       title = \"Dissolved Oxygen vs Fish Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSee code\n# salinity \nggplot(fish_water, aes(x = salinity_ppt_mean, y = total_density_per_m2, color = factor(dredging))) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = TRUE, lwd = 0.7) +\n  scale_color_manual(values = c(\"0\" = \"#A6611A\", \"1\" = \"#5AB4AC\"),\n                     labels = c(\"Pre-Dredging\", \"Post-Dredging\")) +\n  labs(color = \"Dredging Status\",\n       x = \"Average salinity (ppt)\",\n       y = \"Fish density per m2\",\n       title = \"Salinity vs Fish Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe scatter plots do not show a super obvious trend between the water quality variables and the fish density between 2022 and 2023. I did think it was interesting that for the 2022 data, the range of dissolved oxygen and salinity is much more narrow than the 2023 data. I am curious if this might be an indicator to the impacts of the dredging. My hypothesis is the deeper channels caused by the dredging could increase dissolved oxygen by improving water flow and removing organic material and salinity could rise due to enhanced tidal exchange. I can’t tell if that is what happened here by just looking at the data, but it is interesting to consider."
  },
  {
    "objectID": "posts/2024-12-10-fish-stats/index.html#linear-regression-models",
    "href": "posts/2024-12-10-fish-stats/index.html#linear-regression-models",
    "title": "Exploring the Impacts of Dredging on Fish Abundance",
    "section": "Linear Regression Models",
    "text": "Linear Regression Models\n\nModel 1: Multiple linear regressions with binary variable\nDependent variable: fish density\nPredictors: water quality parameters and binary dredging variable\n\n# Linear regression model of water quality parameters + dredging \nabund_model &lt;- lm(total_density_per_m2 ~ temp_c_mean + salinity_ppt_mean + dissolved_oxygen_mean + dredging, data = fish_water)\n\nsummary(abund_model)\n\n\nCall:\nlm(formula = total_density_per_m2 ~ temp_c_mean + salinity_ppt_mean + \n    dissolved_oxygen_mean + dredging, data = fish_water)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.04745 -0.03314 -0.01856 -0.00035  0.48831 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)           -0.2030981  0.2799882  -0.725    0.471\ntemp_c_mean            0.0120304  0.0125339   0.960    0.341\nsalinity_ppt_mean     -0.0008393  0.0026287  -0.319    0.751\ndissolved_oxygen_mean  0.0032059  0.0041702   0.769    0.445\ndredging              -0.0082092  0.0254714  -0.322    0.748\n\nResidual standard error: 0.07801 on 62 degrees of freedom\nMultiple R-squared:  0.02903,   Adjusted R-squared:  -0.03362 \nF-statistic: 0.4634 on 4 and 62 DF,  p-value: 0.7623\n\n\nResults: No significant (less than 0.05) p-values.\n\n\nModel 2: Test for interactions\nTo explore if the effect of water quality differs between pre- and post-dredging periods, I included interaction terms:\nDependent variable: fish density\nPredictors: water quality parameters with dredging interaction\n\n# Linear model with interactions between water quality & dredging\nmodel_interaction &lt;- lm(total_density_per_m2 ~ temp_c_mean * dredging + salinity_ppt_mean * dredging + dissolved_oxygen_mean * dredging, data = fish_water)\n\nsummary(model_interaction)\n\n\nCall:\nlm(formula = total_density_per_m2 ~ temp_c_mean * dredging + \n    salinity_ppt_mean * dredging + dissolved_oxygen_mean * dredging, \n    data = fish_water)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.05947 -0.03000 -0.01833 -0.00420  0.47359 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                    -0.341131   0.427079  -0.799    0.428\ntemp_c_mean                     0.020148   0.019296   1.044    0.301\ndredging                        0.198173   0.602258   0.329    0.743\nsalinity_ppt_mean              -0.003265   0.008626  -0.379    0.706\ndissolved_oxygen_mean           0.012797   0.020352   0.629    0.532\ntemp_c_mean:dredging           -0.011827   0.027544  -0.429    0.669\ndredging:salinity_ppt_mean      0.002842   0.009121   0.312    0.756\ndredging:dissolved_oxygen_mean -0.010560   0.020842  -0.507    0.614\n\nResidual standard error: 0.07962 on 59 degrees of freedom\nMultiple R-squared:  0.03749,   Adjusted R-squared:  -0.0767 \nF-statistic: 0.3283 on 7 and 59 DF,  p-value: 0.9381\n\n\nResults: No siginificant p-values.\nI did not find statistically significant relationships between fish density and water quality parameters (temperature, dissolved oxygen, salinity) or dredging activity. This suggests these factors may not have had a strong or measurable impact on fish density in my dataset.\n\n\nModel 3: Simple it down\nNow, lets see more simply if there is a difference in fish density between the two years separate of the water quality variables:\nDependent variable: fish density\nPredictor: binary dredging variable\n\n# Linear regression of fish density with only dredging as a factor\nmodel_dredging &lt;- lm(total_density_per_m2 ~ dredging, data = fish_water)\nsummary(model_dredging)\n\n\nCall:\nlm(formula = total_density_per_m2 ~ dredging, data = fish_water)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.04130 -0.03134 -0.02207 -0.00260  0.49472 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  0.04130    0.01282   3.222  0.00199 **\ndredging    -0.01582    0.01884  -0.839  0.40429   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0769 on 65 degrees of freedom\nMultiple R-squared:  0.01073,   Adjusted R-squared:  -0.004494 \nF-statistic: 0.7047 on 1 and 65 DF,  p-value: 0.4043\n\n\nResult: p-value is 0.40429. This is not a significant p-value which means we do not have strong evidence that there is a difference in fish abundances between 2022 and 2023."
  },
  {
    "objectID": "posts/2024-12-10-fish-stats/index.html#conclusions",
    "href": "posts/2024-12-10-fish-stats/index.html#conclusions",
    "title": "Exploring the Impacts of Dredging on Fish Abundance",
    "section": "Conclusions",
    "text": "Conclusions\nThe results of my analysis suggest that the factors I considered may not have had a strong or measurable impact on fish density in this dataset. However, this does not necessarily mean there is no relationship—it simply means that, with the current dataset, model, and variables, I did not find evidence to support one. It is likely that other unmeasured variables play a more significant role in influencing fish densities.\nOne limitation of this study is the timing of the sampling. Since the data was collected during an annual field season between July and September, any short-term impacts of dredging may have dissipated by the time of sampling. A more focused dataset, capturing conditions immediately before, during, and after dredging, could provide a clearer picture of its effects. Additionally, the sample size—36 points in 2022 and 31 in 2023—may not have been sufficient for detecting subtle changes in fish density. While larger datasets would provide more robust insights, I now have a much deeper appreciation for the effort required to collect each data point. Having been part of the fieldwork team, I understand the physical and logistical challenges of collecting this data, often under harsh conditions. This perspective reminds me to respect the immense effort behind every dataset used in an analysis.\nFinally, while insignificant p-values can be disappointing, the results of this study might indicate something positive: there may truly be no significant difference in fish densities between the pre- and post-dredging field seasons. This could suggest that dredging did not negatively impact fish populations in the salt marsh. If given more time, it would be valuable to complement this study with an analysis on the sessile organisms that live in the marsh which could not move out of the path of the dredge and may have been more directly affected.\nTo check out more of my analysis, see my GitHub Repository for this project."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jordan Sibley",
    "section": "",
    "text": "Hey! Currently, I am a Master of Environmental Data Science (MEDS) student at the Bren School of Environmental Science & Management. During my time here at Bren, I aim to harness my analytical prowess in data science to drive impactful contributions to scientific research and sustainable solutions for marine coastal ecosystems."
  },
  {
    "objectID": "index.html#jordan-sibley",
    "href": "index.html#jordan-sibley",
    "title": "Jordan Sibley",
    "section": "",
    "text": "Hey! Currently, I am a Master of Environmental Data Science (MEDS) student at the Bren School of Environmental Science & Management. During my time here at Bren, I aim to harness my analytical prowess in data science to drive impactful contributions to scientific research and sustainable solutions for marine coastal ecosystems."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jordan Sibley",
    "section": "Education",
    "text": "Education\nMaster of Environmental Data Science (June 2025) Bren School of Environmental Science & Management\nBachelor of Science in Aquatic Biology (June 2023) University of California, Santa Barbara"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Exploring the Impacts of Dredging on Fish Abundance\n\n\n\nR\n\n\nMEDS\n\n\nStats\n\n\n\nA statistical analysis of fish abundance in a wetland, comparing pre- and post-dredging years\n\n\n\nJordan Sibley\n\n\nDec 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemote Sensing Analysis of the Thomas Fire\n\n\n\nPython\n\n\nMEDS\n\n\nSpatial-Analysis\n\n\n\nThis project aims to explore the area impacted by the Thomas Fire across Ventura and Santa Barbara counties in 2017.\n\n\n\nJordan Sibley\n\n\nDec 3, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]